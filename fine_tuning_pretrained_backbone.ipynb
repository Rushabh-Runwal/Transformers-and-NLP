{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xzf aclImdb_v1.tar.gz\n",
        "!rm -rf aclImdb/train/unsup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcT2qoy2DhKd",
        "outputId": "aa68bfa2-7525-4113-d6cb-5b0530765cca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-05 04:51:10--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  52.9MB/s    in 1.5s    \n",
            "\n",
            "2025-05-05 04:51:12 (52.9 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning a Pretrained Backbone with Keras NLP\n",
        "# --- VERSION WITH OPTIMIZATIONS ---\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras_nlp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "import time\n",
        "import os\n",
        "import math # For ceiling division\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Keras NLP version:\", keras_nlp.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjBkRKgCGjhl",
        "outputId": "85339344-638f-4799-a329-40d5da1d47fe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n",
            "Keras NLP version: 0.18.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Configuration ---\n",
        "# Dataset\n",
        "dataset_dir = \"aclImdb\" # Make sure this directory exists\n",
        "# Model & Sequence Length\n",
        "# Consider smaller models like \"distilbert_base_en_uncased\" if memory is tight\n",
        "# PRESET = \"distilbert_base_en_uncased\"\n",
        "PRESET = \"bert_base_en_uncased\"\n",
        "# Try reducing sequence length (e.g., 256, 128) if memory is an issue and texts allow\n",
        "max_sequence_length = 256\n",
        "# Training Hyperparameters\n",
        "BATCH_SIZE = 16 # Reduced batch size to save memory (adjust based on your GPU)\n",
        "INITIAL_EPOCHS = 2 # Epochs for training the head\n",
        "FINE_TUNE_EPOCHS = 3 # Epochs for fine-tuning the whole model\n",
        "INITIAL_LR = 1e-4 # Learning rate for initial head training\n",
        "END_LR = 1e-7 # End learning rate for decay schedule during fine-tuning\n",
        "# Learning rate for fine-tuning - will be overwritten by schedule if enabled\n",
        "# FINE_TUNE_LR = 5e-5\n",
        "WEIGHT_DECAY = 0.01 # Weight decay for AdamW\n",
        "WARMUP_RATE = 0.1 # Proportion of fine-tuning steps for warmup\n",
        "ENABLE_LR_SCHEDULE = True # Set to False to use fixed FINE_TUNE_LR\n",
        "# Mixed Precision (requires compatible GPU - e.g., T4, V100, A100)\n",
        "ENABLE_MIXED_PRECISION = True\n",
        "\n",
        "# --- (Optional) Enable Mixed Precision Training ---\n",
        "if ENABLE_MIXED_PRECISION:\n",
        "    try:\n",
        "        print(\"\\n=== Enabling Mixed Precision Training (float16) ===\")\n",
        "        keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "        print(\"Mixed precision policy set to 'mixed_float16'.\")\n",
        "        # Note: Ensure your GPU supports float16 operations for performance benefits.\n",
        "    except Exception as e:\n",
        "        print(f\"Could not enable mixed precision: {e}. Continuing with float32.\")\n",
        "        ENABLE_MIXED_PRECISION = False # Fallback if policy setting fails\n",
        "\n",
        "# --- 1. Load and Prepare Dataset ---\n",
        "print(\"\\n=== 1. Loading and Preparing Dataset ===\")\n",
        "if not os.path.exists(dataset_dir):\n",
        "    print(f\"ERROR: Dataset directory '{dataset_dir}' not found.\")\n",
        "    print(\"Please download the IMDB dataset (aclImdb_v1.tar.gz) and extract it.\")\n",
        "    raise FileNotFoundError(f\"Dataset directory '{dataset_dir}' not found.\")"
      ],
      "metadata": {
        "id": "B8dN509fyJSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load raw text data directly into tf.data.Dataset objects\n",
        "train_ds, validation_ds = keras.utils.text_dataset_from_directory(\n",
        "    dataset_dir,\n",
        "    batch_size=BATCH_SIZE, # Use config variable\n",
        "    validation_split=0.2,\n",
        "    subset=\"both\",\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "# Get dataset sizes for LR schedule calculation (more robust than hardcoding)\n",
        "try:\n",
        "    num_train_examples = tf.data.experimental.cardinality(train_ds).numpy() * BATCH_SIZE\n",
        "    num_val_examples = tf.data.experimental.cardinality(validation_ds).numpy() * BATCH_SIZE\n",
        "    # Note: Cardinality might be infinite/unknown if dataset ops aren't fully defined yet.\n",
        "    # If it fails, fall back to info printed during loading (usually reliable here)\n",
        "    if num_train_examples <= 0: raise ValueError(\"Cardinality failed\")\n",
        "    print(f\"Detected {num_train_examples} training examples, {num_val_examples} validation examples.\")\n",
        "except:\n",
        "    print(\"Could not determine dataset size via cardinality, using loading info.\")\n",
        "    # Use info from loading printout (update if your split/dataset changes)\n",
        "    num_train_examples = 40004\n",
        "    num_val_examples = 10001\n",
        "    print(f\"Using assumed sizes: {num_train_examples} training, {num_val_examples} validation.\")\n",
        "\n",
        "\n",
        "print(f\"Training dataset specs: {train_ds.element_spec}\")\n",
        "print(f\"Validation dataset specs: {validation_ds.element_spec}\")\n"
      ],
      "metadata": {
        "id": "9wAB7Y0wyNXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Apply caching and prefetching for performance\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "validation_ds = validation_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# --- 2. Load Pretrained Backbone & Preprocessor ---\n",
        "print(f\"\\n=== 2. Loading Pretrained Backbone & Preprocessor ({PRESET}) ===\")\n",
        "\n",
        "# Use specific preprocessor/backbone based on PRESET\n",
        "if \"distilbert\" in PRESET:\n",
        "    PreprocessorClass = keras_nlp.models.DistilBertPreprocessor\n",
        "    BackboneClass = keras_nlp.models.DistilBertBackbone\n",
        "elif \"bert\" in PRESET:\n",
        "    PreprocessorClass = keras_nlp.models.BertPreprocessor\n",
        "    BackboneClass = keras_nlp.models.BertBackbone\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported preset type: {PRESET}\")\n",
        "\n",
        "preprocessor = PreprocessorClass.from_preset(\n",
        "    PRESET,\n",
        "    sequence_length=max_sequence_length,\n",
        ")\n",
        "backbone = BackboneClass.from_preset(PRESET)\n",
        "\n"
      ],
      "metadata": {
        "id": "WEdsZlOMygGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 3. Build the Fine-tuning Model ---\n",
        "print(\"\\n=== 3. Building the Fine-tuning Model ===\")\n",
        "\n",
        "inputs = keras.Input(shape=(), dtype=\"string\", name=\"text_input\")\n",
        "preprocessed_inputs = preprocessor(inputs)\n",
        "backbone_outputs = backbone(preprocessed_inputs)\n",
        "\n",
        "# Use pooled_output if available and suitable (often pre-processed for classification)\n",
        "# Otherwise, fallback to CLS token from sequence_output\n",
        "if \"pooled_output\" in backbone_outputs:\n",
        "     # Check if pooled_output is valid (might be None for some models/configs)\n",
        "    if backbone_outputs[\"pooled_output\"] is not None:\n",
        "        print(\"Using 'pooled_output' from backbone.\")\n",
        "        sequence_representation = backbone_outputs[\"pooled_output\"]\n",
        "    else: # Fallback if pooled_output is None\n",
        "         print(\"Using 'sequence_output[:, 0, :]' (CLS token) as pooled_output was None.\")\n",
        "         sequence_representation = backbone_outputs[\"sequence_output\"][:, 0, :]\n",
        "else: # Fallback if key doesn't exist\n",
        "    print(\"Using 'sequence_output[:, 0, :]' (CLS token).\")\n",
        "    sequence_representation = backbone_outputs[\"sequence_output\"][:, 0, :]\n",
        "\n",
        "# Add dropout and classification head\n",
        "intermediate_output = keras.layers.Dropout(0.1)(sequence_representation)\n",
        "# Use float32 for the final Dense layer when using mixed precision for stability\n",
        "outputs = keras.layers.Dense(1, activation=\"sigmoid\", name=\"classifier_head\", dtype=tf.float32)(intermediate_output)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "K9CxkI10ybu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTIMIZATION: Use AdamW optimizer\n",
        "optimizer_tuned = keras.optimizers.AdamW(\n",
        "    learning_rate=final_learning_rate, # Use schedule or fixed LR\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    # clipnorm=1.0 # Optional: gradient clipping\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer_tuned,\n",
        "    loss=loss_fn, # Reuse loss from before\n",
        "    metrics=[\"accuracy\"],\n",
        "    jit_compile=False # Keep False\n",
        ")\n",
        "\n",
        "print(\"\\nContinuing training (fine-tuning) with unfrozen backbone...\")\n",
        "total_epochs = INITIAL_EPOCHS + FINE_TUNE_EPOCHS\n",
        "start_time = time.time()\n",
        "history_unfrozen = model.fit(\n",
        "    train_ds,\n",
        "    epochs=total_epochs,\n",
        "    initial_epoch=INITIAL_EPOCHS, # Start counting from here\n",
        "    validation_data=validation_ds,\n",
        ")\n",
        "end_time = time.time()\n",
        "print(f\"Time taken for fine-tuning: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "jUFEwBqvyXSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 4. Initial Training with Frozen Backbone ---\n",
        "print(\"\\n=== 4. Training with Frozen Backbone (Warm-up) ===\")\n",
        "print(\"Initially freezing the backbone layers...\")\n",
        "backbone.trainable = False\n",
        "\n",
        "# OPTIMIZATION: Use AdamW optimizer\n",
        "optimizer_frozen = keras.optimizers.AdamW(\n",
        "    learning_rate=INITIAL_LR,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    # clipnorm=1.0 # Optional: gradient clipping\n",
        ")\n",
        "# Loss needs to be float32 when using mixed precision\n",
        "loss_fn = keras.losses.BinaryCrossentropy(from_logits=False) # Sigmoid applied\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer_frozen,\n",
        "    loss=loss_fn,\n",
        "    metrics=[\"accuracy\"],\n",
        "    jit_compile=False # Keep False to avoid XLA string issues\n",
        ")\n",
        "\n",
        "print(f\"Training head for {INITIAL_EPOCHS} epochs...\")\n",
        "start_time = time.time()\n",
        "history_frozen = model.fit(\n",
        "    train_ds,\n",
        "    epochs=INITIAL_EPOCHS,\n",
        "    validation_data=validation_ds,\n",
        ")\n",
        "end_time = time.time()\n",
        "print(f\"Time taken for training with frozen backbone: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "# --- 5. Fine-tuning the Entire Model ---\n",
        "print(\"\\n=== 5. Fine-tuning the Entire Model ===\")\n",
        "print(\"Unfreezing the backbone layers...\")\n",
        "backbone.trainable = True\n",
        "\n",
        "# OPTIMIZATION: Calculate steps for LR schedule\n",
        "steps_per_epoch = math.ceil(num_train_examples / BATCH_SIZE)\n",
        "fine_tune_steps = steps_per_epoch * FINE_TUNE_EPOCHS\n",
        "warmup_steps = int(WARMUP_RATE * fine_tune_steps)\n",
        "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
        "print(f\"Total fine-tune steps: {fine_tune_steps}\")\n",
        "print(f\"Warmup steps: {warmup_steps}\")\n",
        "\n",
        "if ENABLE_LR_SCHEDULE:\n",
        "    print(\"Using learning rate schedule with linear decay and warmup.\")\n",
        "    # Start LR is the peak LR after warmup\n",
        "    start_lr_schedule = INITIAL_LR # Or use FINE_TUNE_LR if defined separately\n",
        "\n",
        "    # Create Polynomial Decay schedule (often used, approximates linear)\n",
        "    lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "        initial_learning_rate=start_lr_schedule,\n",
        "        decay_steps=fine_tune_steps - warmup_steps, # Steps after warmup\n",
        "        end_learning_rate=END_LR,\n",
        "        power=1.0, # Power 1.0 is linear decay\n",
        "    )\n",
        "\n",
        "    # Apply warmup phase\n",
        "    if warmup_steps > 0:\n",
        "        warmup_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "             initial_learning_rate=1e-9, # Start from near zero\n",
        "             decay_steps=warmup_steps,\n",
        "             end_learning_rate=start_lr_schedule,\n",
        "             power=1.0\n",
        "        )\n",
        "        # Combine warmup and decay\n",
        "        # Note: Keras schedules are based on step counts directly\n",
        "        # We need to manually switch based on step or use a custom schedule wrapper\n",
        "        # For simplicity here, we might just use AdamW's internal schedule capabilities if simpler,\n",
        "        # or accept that PolynomialDecay starts decay immediately after warmup peak.\n",
        "        # A simpler approximation using AdamW's beta_1 for warmup isn't standard.\n",
        "        # Let's stick to the PolynomialDecay but acknowledge the warmup integration isn't perfect via built-ins easily.\n",
        "        # A more robust way involves custom schedule objects or optimizers with built-in warmup+decay.\n",
        "        # Let's just use the decay part for now, starting from peak LR, assuming warmup happens implicitly or via optimizer properties\n",
        "        # Reverting to a simpler setup for PolynomialDecay post-warmup:\n",
        "        lr_schedule_post_warmup = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "             initial_learning_rate=start_lr_schedule, # Peak LR\n",
        "             decay_steps=fine_tune_steps - warmup_steps,\n",
        "             end_learning_rate=END_LR,\n",
        "             power=1.0\n",
        "        )\n",
        "        # We'll pass this to AdamW. Warmup needs custom handling or specific optimizer support usually.\n",
        "        # Keras's AdamW doesn't have built-in linear warmup + decay schedule.\n",
        "        # For demonstration, we proceed with decay from peak LR. Actual warmup requires more code.\n",
        "        print(f\"WARNING: Using PolynomialDecay starting from peak LR ({start_lr_schedule}). Proper warmup requires custom schedule.\")\n",
        "        final_learning_rate = lr_schedule_post_warmup # Use the decay schedule\n",
        "else:\n",
        "    print(f\"Using fixed learning rate: {INITIAL_LR}\") # Use initial LR as FINE_TUNE_LR wasn't set\n",
        "    final_learning_rate = INITIAL_LR\n"
      ],
      "metadata": {
        "id": "sMcym6sMyipT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --- 6. Visualize Training History ---\n",
        "# (Plotting code remains the same as previous version)\n",
        "print(\"\\n=== 6. Visualizing Training History ===\")\n",
        "history = {}\n",
        "try:\n",
        "    # Combine histories only if both phases ran successfully\n",
        "    history[\"accuracy\"] = history_frozen.history[\"accuracy\"] + history_unfrozen.history[\"accuracy\"]\n",
        "    history[\"val_accuracy\"] = history_frozen.history[\"val_accuracy\"] + history_unfrozen.history[\"val_accuracy\"]\n",
        "    history[\"loss\"] = history_frozen.history[\"loss\"] + history_unfrozen.history[\"loss\"]\n",
        "    history[\"val_loss\"] = history_frozen.history[\"val_loss\"] + history_unfrozen.history[\"val_loss\"]\n",
        "except KeyError as e:\n",
        "     print(f\"Warning: Could not combine histories completely due to missing key: {e}. Plotting might be incomplete.\")\n",
        "     history = history_unfrozen.history # Plot only the fine-tuning part if frozen part failed\n",
        "\n",
        "if history and \"accuracy\" in history: # Check if history is usable\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history[\"accuracy\"], label=\"Training Accuracy\")\n",
        "    plt.plot(history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
        "    plt.axvline(x=INITIAL_EPOCHS-1, color=\"r\", linestyle=\"--\", label=\"Start Fine Tuning\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.ylim([0, 1]) # Standardize y-axis for accuracy\n",
        "    plt.legend()\n",
        "    plt.title(\"Training and Validation Accuracy\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history[\"loss\"], label=\"Training Loss\")\n",
        "    plt.plot(history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    plt.axvline(x=INITIAL_EPOCHS-1, color=\"r\", linestyle=\"--\", label=\"Start Fine Tuning\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Training and Validation Loss\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Skipping plotting as history data is missing or incomplete.\")\n",
        "\n",
        "\n",
        "# --- 7. Final Model Evaluation ---\n",
        "print(\"\\n=== 7. Final Model Evaluation ===\")\n",
        "loss, accuracy = model.evaluate(validation_ds)\n",
        "# If using mixed precision, accuracy might be float16, convert for display\n",
        "accuracy = float(accuracy)\n",
        "loss = float(loss)\n",
        "print(f\"Final Validation Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Final Validation Loss: {loss:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "wZsjDP9pyR67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 8. Testing on New Examples ---\n",
        "# (Testing code remains the same)\n",
        "print(\"\\n=== 8. Testing on New Examples ===\")\n",
        "test_examples = [\n",
        "    \"This movie was fantastic! I really enjoyed the plot and the acting was superb.\",\n",
        "    \"What a waste of time. Poor acting, terrible script, and boring storyline.\",\n",
        "    \"The movie had good special effects but the story was somewhat confusing.\",\n",
        "    \"I fell asleep halfway through the movie.\",\n",
        "    \"The characters were well-developed and the dialogue was engaging.\"\n",
        "]\n",
        "predictions = model.predict(tf.constant(test_examples))\n",
        "for i, (text, pred) in enumerate(zip(test_examples, predictions)):\n",
        "    pred_value = float(pred[0]) # Ensure float for comparison\n",
        "    sentiment = \"Positive\" if pred_value >= 0.5 else \"Negative\"\n",
        "    confidence = pred_value if sentiment == \"Positive\" else 1 - pred_value\n",
        "    print(f\"\\nExample {i+1}: {text}\")\n",
        "    print(f\"Prediction: {sentiment} (confidence: {confidence:.4f})\")\n",
        "\n",
        "\n",
        "# --- 9. Saving the Fine-tuned Model ---\n",
        "# (Saving code remains largely the same)\n",
        "print(\"\\n=== 9. Saving the Fine-tuned Model ===\")\n",
        "model_save_path = f\"./{PRESET}_imdb_finetuned.keras\" # Dynamic name based on preset\n",
        "try:\n",
        "    model.save(model_save_path)\n",
        "    print(f\"Model saved successfully to {model_save_path}\")\n",
        "\n",
        "    # Optional: Test loading the saved model\n",
        "    print(\"\\nTesting loading the saved model...\")\n",
        "    # When loading mixed precision models, custom objects usually not needed\n",
        "    # but good practice if complex layers were added\n",
        "    loaded_model = keras.models.load_model(model_save_path)\n",
        "    print(\"Model loaded successfully.\")\n",
        "    loaded_pred = loaded_model.predict([\"A quick test after loading.\"])\n",
        "    print(f\"Test prediction with loaded model: {float(loaded_pred[0][0]):.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error saving or loading model: {e}\")\n",
        "\n",
        "\n",
        "# --- Tips ---\n",
        "# (Tips remain the same)\n",
        "print(\"\\n=== Tips for Improving Fine-tuning Results ===\")\n",
        "# ... (tips copied from previous version) ...\n",
        "print(\"1. Use longer training with more epochs (monitor validation loss for overfitting)\")\n",
        "print(\"2. Experiment with different learning rates or learning rate schedules (e.g., linear decay)\")\n",
        "print(\"3. Try larger batch sizes if your GPU memory allows (or use gradient accumulation)\")\n",
        "print(\"4. Explore gradual unfreezing (unfreeze layer groups incrementally)\")\n",
        "print(\"5. Apply techniques like weight decay (AdamW helps here)\")\n",
        "print(\"6. Use longer maximum sequence length if needed and memory allows\")\n",
        "print(\"7. Consider data augmentation techniques for text (e.g., back-translation, synonym replacement)\")\n",
        "print(\"8. Try different pretrained backbones (e.g., RoBERTa, DistilBERT, ELECTRA)\")\n",
        "\n",
        "\n",
        "print(f\"\\nComplete! Optimized fine-tuning script finished for {PRESET}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-u-dja25Uo4x",
        "outputId": "92b88313-ccdb-4090-9b08-5371f70294bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== 8. Testing on New Examples ===\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "\n",
            "Example 1: This movie was fantastic! I really enjoyed the plot and the acting was superb.\n",
            "Prediction: Positive (confidence: 0.5115)\n",
            "\n",
            "Example 2: What a waste of time. Poor acting, terrible script, and boring storyline.\n",
            "Prediction: Positive (confidence: 0.5115)\n",
            "\n",
            "Example 3: The movie had good special effects but the story was somewhat confusing.\n",
            "Prediction: Positive (confidence: 0.5115)\n",
            "\n",
            "Example 4: I fell asleep halfway through the movie.\n",
            "Prediction: Positive (confidence: 0.5116)\n",
            "\n",
            "Example 5: The characters were well-developed and the dialogue was engaging.\n",
            "Prediction: Positive (confidence: 0.5115)\n",
            "\n",
            "=== 9. Saving the Fine-tuned Model ===\n",
            "Model saved successfully to ./bert_base_en_uncased_imdb_finetuned.keras\n",
            "\n",
            "Testing loading the saved model...\n",
            "Error saving or loading model: Exception encountered when calling BertTextClassifierPreprocessor.call().\n",
            "\n",
            "\u001b[1mCould not automatically infer the output shape / dtype of 'bert_text_classifier_preprocessor' (of type BertTextClassifierPreprocessor). Either the `BertTextClassifierPreprocessor.call()` method is incorrect, or you need to implement the `BertTextClassifierPreprocessor.compute_output_spec() / compute_output_shape()` method. Error encountered:\n",
            "\n",
            "Exception encountered when calling BertTokenizer.call().\n",
            "\n",
            "\u001b[1mNo vocabulary has been set for WordPieceTokenizer. Make sure to pass a `vocabulary` argument when creating the layer.\u001b[0m\n",
            "\n",
            "Arguments received by BertTokenizer.call():\n",
            "  • inputs=tf.Tensor(shape=(None,), dtype=string)\n",
            "  • args=<class 'inspect._empty'>\n",
            "  • training=None\n",
            "  • kwargs=<class 'inspect._empty'>\u001b[0m\n",
            "\n",
            "Arguments received by BertTextClassifierPreprocessor.call():\n",
            "  • args=('<KerasTensor shape=(None,), dtype=string, sparse=False, name=text_input>',)\n",
            "  • kwargs=<class 'inspect._empty'>\n",
            "\n",
            "=== Tips for Improving Fine-tuning Results ===\n",
            "1. Use longer training with more epochs (monitor validation loss for overfitting)\n",
            "2. Experiment with different learning rates or learning rate schedules (e.g., linear decay)\n",
            "3. Try larger batch sizes if your GPU memory allows (or use gradient accumulation)\n",
            "4. Explore gradual unfreezing (unfreeze layer groups incrementally)\n",
            "5. Apply techniques like weight decay (AdamW helps here)\n",
            "6. Use longer maximum sequence length if needed and memory allows\n",
            "7. Consider data augmentation techniques for text (e.g., back-translation, synonym replacement)\n",
            "8. Try different pretrained backbones (e.g., RoBERTa, DistilBERT, ELECTRA)\n",
            "\n",
            "Complete! Optimized fine-tuning script finished for bert_base_en_uncased.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vYrayXyYLKJL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}