{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uSHeiaX2FgSV"
      },
      "outputs": [],
      "source": [
        "# Building and Training a Transformer from Scratch with Keras\n",
        "# This notebook shows how to build a transformer model for text classification\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import string\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load the IMDB dataset for sentiment analysis\n",
        "print(\"\\n=== Loading and Preparing Dataset ===\")\n",
        "\n",
        "vocab_size = 20000  # Top N words to use in vocabulary\n",
        "max_sequence_length = 200  # Maximum length of sequences\n",
        "\n",
        "# Load the dataset\n",
        "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(\n",
        "    num_words=vocab_size,\n",
        "    maxlen=max_sequence_length\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXvs39DMyol0",
        "outputId": "4febc2ff-5b2d-48d3-fe54-2bb0897d718e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n",
            "\n",
            "=== Loading and Preparing Dataset ===\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"Training examples: {len(x_train)}\")\n",
        "print(f\"Validation examples: {len(x_val)}\")\n",
        "\n",
        "# Get the word index mapping\n",
        "word_index = keras.datasets.imdb.get_word_index()\n",
        "# Convert to lowercase and offset indices by 3 (for padding, start, unknown tokens)\n",
        "word_index = {word: (idx + 3) for word, idx in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "\n",
        "# Create reverse word index\n",
        "reverse_word_index = {idx: word for word, idx in word_index.items()}\n",
        "\n",
        "# Function to decode the integer sequences back to text\n",
        "def decode_review(sequence):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in sequence])\n",
        "\n",
        "# Print some examples\n",
        "print(\"\\nSample training examples:\")\n",
        "for i in range(3):\n",
        "    sentiment = \"Positive\" if y_train[i] == 1 else \"Negative\"\n",
        "    print(f\"Example {i+1}: {sentiment}\")\n",
        "    text = decode_review(x_train[i])\n",
        "    print(f\"Text: {text[:100]}...\")\n",
        "    print()\n",
        "\n",
        "# Function to pad the sequences\n",
        "def pad_sequences(sequences, maxlen):\n",
        "    return tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        sequences, maxlen=maxlen, padding='post'\n",
        "    )\n",
        "\n",
        "# Pad the sequences\n",
        "x_train_padded = pad_sequences(x_train, max_sequence_length)\n",
        "x_val_padded = pad_sequences(x_val, max_sequence_length)\n",
        "\n",
        "print(f\"Shape of training data: {x_train_padded.shape}\")\n",
        "print(f\"Shape of validation data: {x_val_padded.shape}\")\n",
        "\n",
        "# Convert to TensorFlow Dataset for better performance\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_padded, y_train))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val_padded, y_val))\n",
        "\n",
        "# Batch and prefetch for better performance\n",
        "batch_size = 64\n",
        "train_dataset = train_dataset.shuffle(buffer_size=10000).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Now let's define the transformer model components from scratch\n",
        "print(\"\\n=== Building Transformer Components from Scratch ===\")\n",
        "\n",
        "# Positional Encoding Layer\n",
        "class PositionalEncoding(layers.Layer):\n",
        "    def __init__(self, position, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "    def get_angles(self, position, i, d_model):\n",
        "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "        return position * angles\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(\n",
        "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "            d_model=d_model\n",
        "        )\n",
        "\n",
        "        # Apply sine to even indices in the array\n",
        "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "        # Apply cosine to odd indices\n",
        "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
        "\n",
        "# Multi-Head Attention Layer\n",
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "    # Calculate dot product of query and key\n",
        "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "    # Scale matmul_qk\n",
        "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "    # Add mask if provided\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # Apply softmax to get attention weights\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    # Apply attention weights to value\n",
        "    output = tf.matmul(attention_weights, value)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "class MultiHeadAttention(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = layers.Dense(d_model)\n",
        "        self.wk = layers.Dense(d_model)\n",
        "        self.wv = layers.Dense(d_model)\n",
        "\n",
        "        self.dense = layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        # Reshape to (batch_size, seq_len, num_heads, depth)\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        # Transpose to (batch_size, num_heads, seq_len, depth)\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask=None):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len_q, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len_k, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len_v, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # Calculate attention\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask\n",
        "        )\n",
        "\n",
        "        # Transpose back to (batch_size, seq_len_q, num_heads, depth)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        # Concatenate heads and put through final dense layer\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "\n",
        "        output = self.dense(concat_attention)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "# Feed Forward Network\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return keras.Sequential([\n",
        "        layers.Dense(dff, activation='relu'),\n",
        "        layers.Dense(d_model)\n",
        "    ])\n",
        "\n",
        "# Encoder Layer\n",
        "class EncoderLayer(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask=None):\n",
        "        # Multi-head attention part\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        # Feed forward part\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2\n",
        "\n",
        "# Transformer Encoder\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.enc_layers = [\n",
        "            EncoderLayer(d_model, num_heads, dff, rate)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "\n",
        "        self.dropout = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # Embedding and position encoding\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x = self.pos_encoding(x)\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        # Encoder layers\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training=training, mask=mask)\n",
        "\n",
        "\n",
        "        return x  # (batch_size, seq_len, d_model)\n",
        "\n",
        "# Now, let's build the complete transformer model for text classification\n",
        "print(\"\\n=== Building the Complete Transformer Model ===\")\n",
        "\n",
        "def create_transformer_classifier(\n",
        "    vocab_size,\n",
        "    num_layers,\n",
        "    d_model,\n",
        "    num_heads,\n",
        "    dff,\n",
        "    max_sequence_length,\n",
        "    dropout_rate=0.1,\n",
        "):\n",
        "    # Define inputs\n",
        "    inputs = layers.Input(shape=(max_sequence_length,))\n",
        "\n",
        "    # Create padding mask to mask out padded tokens\n",
        "    # We don't actually need padding mask for classification task where we just use the [CLS] token\n",
        "    # But we'll keep it for completeness and potential model extension\n",
        "    # Define inputs\n",
        "    inputs = layers.Input(shape=(max_sequence_length,))\n",
        "\n",
        "# Wrap the mask logic in a Lambda layer\n",
        "    padding_mask = layers.Lambda(lambda x: tf.cast(tf.math.equal(x, 0), tf.float32))(inputs)\n",
        "    padding_mask = layers.Lambda(lambda x: x[:, tf.newaxis, tf.newaxis, :])(padding_mask)\n",
        "\n",
        "\n",
        "    # Transformer encoder\n",
        "    encoder = TransformerEncoder(\n",
        "        num_layers=num_layers,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dff=dff,\n",
        "        input_vocab_size=vocab_size,\n",
        "        maximum_position_encoding=max_sequence_length,\n",
        "        rate=dropout_rate\n",
        "    )\n",
        "\n",
        "    # Get encoder output\n",
        "    encoder_output = encoder(inputs, training=True, mask=padding_mask)\n",
        "\n",
        "    # Global average pooling across sequence dimension for classification\n",
        "    pooled_output = layers.GlobalAveragePooling1D()(encoder_output)\n",
        "\n",
        "    # Add a dropout layer for regularization\n",
        "    pooled_output = layers.Dropout(dropout_rate)(pooled_output)\n",
        "\n",
        "    # Final dense layer for classification\n",
        "    outputs = layers.Dense(1, activation='sigmoid')(pooled_output)\n",
        "\n",
        "    # Create the model\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Set model hyperparameters\n",
        "num_layers = 2  # Number of transformer encoder layers\n",
        "d_model = 128    # Embedding dimension\n",
        "num_heads = 8    # Number of attention heads\n",
        "dff = 512       # Feed forward network dimension\n",
        "dropout_rate = 0.1\n",
        "\n",
        "# Create the transformer model\n",
        "transformer_model = create_transformer_classifier(\n",
        "    vocab_size=vocab_size,\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    max_sequence_length=max_sequence_length,\n",
        "    dropout_rate=dropout_rate\n",
        ")\n",
        "\n",
        "# Print model summary\n",
        "transformer_model.summary()\n",
        "\n",
        "# Compile the model\n",
        "print(\"\\n=== Compiling the Model ===\")\n",
        "transformer_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss=keras.losses.BinaryCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Define callbacks\n",
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=2,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"\\n=== Training the Transformer Model ===\")\n",
        "epochs = 5\n",
        "\n",
        "start_time = time.time()\n",
        "history = transformer_model.fit(\n",
        "    train_dataset,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[early_stopping, reduce_lr])\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Time taken for training: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "print(\"\\n=== Evaluating the Model ===\")\n",
        "evaluation = transformer_model.evaluate(val_dataset)\n",
        "print(f\"Validation Loss: {evaluation[0]:.4f}\")\n",
        "print(f\"Validation Accuracy: {evaluation[1]:.4f}\")\n",
        "\n",
        "# Test on some examples\n",
        "print(\"\\n=== Testing on New Examples ===\")\n",
        "\n",
        "# Function to preprocess new text\n",
        "def preprocess_text(text):\n",
        "    # Convert text to word indices\n",
        "    words = text.lower().split()\n",
        "    sequence = []\n",
        "    for word in words:\n",
        "        if word in word_index:\n",
        "            sequence.append(word_index[word])\n",
        "        else:\n",
        "            sequence.append(word_index[\"<UNK>\"])\n",
        "    # Pad sequence\n",
        "    padded_sequence = pad_sequences([sequence], max_sequence_length)\n",
        "    return padded_sequence\n",
        "\n",
        "# Test examples\n",
        "test_examples = [\n",
        "    \"This movie was fantastic! I really enjoyed the plot and the acting was superb.\",\n",
        "    \"What a waste of time. Poor acting, terrible script, and boring storyline.\",\n",
        "    \"The movie had good special effects but the story was somewhat confusing.\",\n",
        "    \"I fell asleep halfway through the movie.\",\n",
        "    \"The characters were well-developed and the dialogue was engaging.\"\n",
        "]\n",
        "\n",
        "for i, text in enumerate(test_examples):\n",
        "    # Preprocess the text\n",
        "    processed_text = preprocess_text(text)\n",
        "    # Make prediction\n",
        "    prediction = transformer_model.predict(processed_text)[0][0]\n",
        "    sentiment = \"Positive\" if prediction >= 0.5 else \"Negative\"\n",
        "    confidence = prediction if sentiment == \"Positive\" else 1 - prediction\n",
        "    print(f\"Example {i+1}: {text}\")\n",
        "    print(f\"Prediction: {sentiment} (confidence: {confidence:.4f})\")\n",
        "    print()\n",
        "\n"
      ],
      "metadata": {
        "id": "Kd09bx6Cy-MH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "print(\"\\n=== Saving the Model ===\")\n",
        "model_save_path = \"./transformer_sentiment_classifier.keras\"\n",
        "transformer_model.save(model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")\n",
        "\n",
        "# Let's write a visualization function for the attention weights\n",
        "print(\"\\n=== Attention Visualization Function ===\")\n",
        "print(\"Note: To visualize attention weights, you would need to modify the model to return attention weights\")\n",
        "\n",
        "def visualize_attention(model, text, layer_index=0, head_index=0):\n",
        "    \"\"\"\n",
        "    Function to visualize attention weights for a given text.\n",
        "    Note: This is a template function. The actual implementation would require\n",
        "    modifying the transformer model to return attention weights.\n",
        "    \"\"\"\n",
        "    print(\"Attention visualization function template:\")\n",
        "    print(\"1. Preprocess the input text\")\n",
        "    print(\"2. Run the model and extract attention weights\")\n",
        "    print(\"3. Plot the attention matrix as a heatmap\")\n",
        "    print(\"4. Display the words and their attention scores\")\n",
        "\n",
        "    # Example code (not functional without model modifications):\n",
        "    \"\"\"\n",
        "    # Preprocess text\n",
        "    processed_text = preprocess_text(text)\n",
        "\n",
        "    # Get attention weights (assuming model was modified to return them)\n",
        "    _, attention_weights = model(processed_text, training=False)\n",
        "\n",
        "    # Extract weights for the specified layer and head\n",
        "    attention = attention_weights[layer_index][0, head_index].numpy()\n",
        "\n",
        "    # Create a heatmap\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(attention, cmap='viridis')\n",
        "\n",
        "    # Add words as axis labels\n",
        "    words = text.split()\n",
        "    plt.xticks(range(len(words)), words, rotation=90)\n",
        "    plt.yticks(range(len(words)), words)\n",
        "\n",
        "    plt.colorbar()\n",
        "    plt.title(f'Attention weights for layer {layer_index}, head {head_index}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \"\"\"\n",
        "\n",
        "print(\"\\n=== Summary and Next Steps ===\")\n",
        "print(\"You've successfully built and trained a transformer model from scratch for sentiment analysis!\")\n",
        "print(\"Possible next steps:\")\n",
        "print(\"1. Try different hyperparameters (more layers, different learning rates, etc.)\")\n",
        "print(\"2. Modify the model architecture (e.g., add residual connections)\")\n",
        "print(\"3. Use a larger dataset or pretrained word embeddings\")\n",
        "print(\"4. Implement attention visualization\")\n",
        "print(\"5. Try different tasks like text generation or translation\")\n",
        "print(\"6. Add more sophisticated regularization techniques\")\n",
        "\n",
        "print(\"\\nComplete! You've successfully built a transformer from scratch for text classification.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAesh67GGa8S",
        "outputId": "49faab8d-3537-42dc-ed76-1a0836f26c4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Saving the Model ===\n",
            "Model saved to ./transformer_sentiment_classifier.keras\n",
            "\n",
            "=== Attention Visualization Function ===\n",
            "Note: To visualize attention weights, you would need to modify the model to return attention weights\n",
            "\n",
            "=== Summary and Next Steps ===\n",
            "You've successfully built and trained a transformer model from scratch for sentiment analysis!\n",
            "Possible next steps:\n",
            "1. Try different hyperparameters (more layers, different learning rates, etc.)\n",
            "2. Modify the model architecture (e.g., add residual connections)\n",
            "3. Use a larger dataset or pretrained word embeddings\n",
            "4. Implement attention visualization\n",
            "5. Try different tasks like text generation or translation\n",
            "6. Add more sophisticated regularization techniques\n",
            "\n",
            "Complete! You've successfully built a transformer from scratch for text classification.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OHTN1rcBI215"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}